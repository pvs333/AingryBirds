# Angry Birds AI/ML - System Architecture

## System Overview Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         UNITY GAME ENGINE                                â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    GAME ENVIRONMENT                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚  â”‚ Bird â”‚  â”‚ Pig  â”‚  â”‚Brick â”‚  â”‚Groundâ”‚  â”‚Sling â”‚             â”‚   â”‚
â”‚  â”‚  â”‚      â”‚  â”‚      â”‚  â”‚      â”‚  â”‚      â”‚  â”‚Shot  â”‚             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â–² â”‚                                         â”‚
â”‚                              â”‚ â”‚ Physics Events                          â”‚
â”‚                              â”‚ â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    ML SYSTEM (C#)                                â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚  â”‚  â”‚ Step 1: Collectorâ”‚  â”‚ Step 2: Processorâ”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ GameStateCollectorâ”‚ â”‚ DataPreprocessor â”‚                    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚  â”‚           â”‚                      â”‚                               â”‚   â”‚
â”‚  â”‚           â–¼                      â–¼                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚  â”‚  â”‚Step 3: State Vec â”‚  â”‚ Step 4: Actions  â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚StateVectorConvertâ”‚  â”‚  MLAgentBrain    â”‚â—„â”€â”€â”€â”               â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚               â”‚   â”‚
â”‚  â”‚           â”‚                      â”‚               â”‚               â”‚   â”‚
â”‚  â”‚           â”‚                      â–¼               â”‚               â”‚   â”‚
â”‚  â”‚           â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚               â”‚   â”‚
â”‚  â”‚           â”‚          â”‚Step 5: Execute   â”‚       â”‚               â”‚   â”‚
â”‚  â”‚           â”‚          â”‚ Physics Simulatorâ”‚       â”‚               â”‚   â”‚
â”‚  â”‚           â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚               â”‚   â”‚
â”‚  â”‚           â”‚                   â”‚                 â”‚               â”‚   â”‚
â”‚  â”‚           â”‚                   â–¼                 â”‚               â”‚   â”‚
â”‚  â”‚           â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚               â”‚   â”‚
â”‚  â”‚           â”‚          â”‚Step 6: Rewards   â”‚      â”‚               â”‚   â”‚
â”‚  â”‚           â”‚          â”‚ RewardCalculator â”‚      â”‚               â”‚   â”‚
â”‚  â”‚           â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚               â”‚   â”‚
â”‚  â”‚           â”‚                   â”‚                 â”‚               â”‚   â”‚
â”‚  â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚  â”‚                               â”‚                                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚         Steps 7-8: Training Manager                     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Episode Manager  â”‚  â”‚  Checkpoint Saver â”‚           â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                           â”‚                                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚         Step 10: Performance Monitor                    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  Metrics Tracker  â”‚  â”‚  Alert System    â”‚           â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ Network Communication (TCP/Socket)
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PYTHON ML BACKEND                                â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Step 7: Policy Network                        â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚     Input (64 dims) â†’ [256] â†’ [256] â†’ [128] â†’ Output (2 dims)  â”‚   â”‚
â”‚  â”‚                         â†“       â†“       â†“                        â”‚   â”‚
â”‚  â”‚                      ReLU    ReLU    ReLU                        â”‚   â”‚
â”‚  â”‚                         â†“       â†“       â†“                        â”‚   â”‚
â”‚  â”‚                    Dropout  Dropout  Dropout                     â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚     Actor Head: [angle, force]                                   â”‚   â”‚
â”‚  â”‚     Critic Head: [state_value]                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Step 8: Training Loop                         â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  1. Collect Experience â†’ 2. Store in Buffer â†’ 3. Sample Batch   â”‚   â”‚
â”‚  â”‚         â†“                                            â†“            â”‚   â”‚
â”‚  â”‚  4. Calculate Loss â† 5. Backpropagation â† 6. Update Weights     â”‚   â”‚
â”‚  â”‚         â†“                                                         â”‚   â”‚
â”‚  â”‚  7. Log Metrics â†’ 8. Save Checkpoint â†’ 9. Next Episode          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Support Components                            â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚Replay Buffer â”‚  â”‚  TensorBoard  â”‚  â”‚   Logger     â”‚          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Episode    â”‚
â”‚   Start     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: COLLECT STATE                                   â”‚
â”‚                                                          â”‚
â”‚  Raw Data:                                              â”‚
â”‚  â€¢ Bird Position: (x, y, type)                         â”‚
â”‚  â€¢ Target Positions: [(xâ‚, yâ‚), (xâ‚‚, yâ‚‚), ...]        â”‚
â”‚  â€¢ Obstacles: [{pos, health, type}, ...]               â”‚
â”‚  â€¢ Previous Attempts: [attemptâ‚, attemptâ‚‚, ...]        â”‚
â”‚  â€¢ Physics: {gravity, time}                            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: EXTRACT FEATURES                                â”‚
â”‚                                                          â”‚
â”‚  Feature Set:                                           â”‚
â”‚  â€¢ Distances: [near, far, avg] â†’ [12.3, 45.6, 28.9]   â”‚
â”‚  â€¢ Angles: [near, far] â†’ [45Â°, -30Â°]                   â”‚
â”‚  â€¢ Obstacles: [density, count, health] â†’ [0.7, 12, 85] â”‚
â”‚  â€¢ Targets: [count, clustering] â†’ [3, 0.8]             â”‚
â”‚  â€¢ History: [attempts, success_rate] â†’ [2, 0.5]        â”‚
â”‚                                                          â”‚
â”‚  Normalized: All values â†’ [-1, 1] or [0, 1]            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: CREATE STATE VECTOR                             â”‚
â”‚                                                          â”‚
â”‚  State Vector [64 dimensions]:                          â”‚
â”‚  [0.23, -0.45, 0.67, ..., 0.12, 0.89, -0.34]          â”‚
â”‚   â†‘      â†‘      â†‘         â†‘      â†‘      â†‘             â”‚
â”‚   â”‚      â”‚      â”‚         â”‚      â”‚      â””â”€ Obstacle 8  â”‚
â”‚   â”‚      â”‚      â”‚         â”‚      â””â”€ Target 10          â”‚
â”‚   â”‚      â”‚      â”‚         â””â”€ Bird features             â”‚
â”‚   â”‚      â”‚      â””â”€ Angle features                      â”‚
â”‚   â”‚      â””â”€ Distance features                          â”‚
â”‚   â””â”€ Core features                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: SELECT ACTION                                   â”‚
â”‚                                                          â”‚
â”‚  if random() < exploration_rate:                        â”‚
â”‚      action = random_action()          [EXPLORE]        â”‚
â”‚  else:                                                  â”‚
â”‚      action = policy_network(state)    [EXPLOIT]        â”‚
â”‚                                                          â”‚
â”‚  Action: [angle: 45.6Â°, force: 0.78]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: EXECUTE ACTION                                  â”‚
â”‚                                                          â”‚
â”‚  1. Calculate pull position from (angle, force)         â”‚
â”‚  2. Pull slingshot back                                 â”‚
â”‚  3. Launch bird with velocity                           â”‚
â”‚  4. Simulate physics                                    â”‚
â”‚  5. Wait for collision/settling                         â”‚
â”‚                                                          â”‚
â”‚  Result: {hit: true, damage: 250, pigs: 1, ...}        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: CALCULATE REWARD                                â”‚
â”‚                                                          â”‚
â”‚  reward = 0                                             â”‚
â”‚  if hit_target: reward += 100                           â”‚
â”‚  reward += damage * 0.5  â†’ +125                         â”‚
â”‚  reward += pigs_killed * 200  â†’ +200                    â”‚
â”‚  reward += time_penalty  â†’ -5                           â”‚
â”‚                                                          â”‚
â”‚  Total Reward: 420                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: UPDATE POLICY (Python)                          â”‚
â”‚                                                          â”‚
â”‚  Experience: (state, action, reward, next_state)        â”‚
â”‚                                                          â”‚
â”‚  1. Predict value: V(s) = critic(state)                 â”‚
â”‚  2. Calculate advantage: A = reward + Î³*V(s') - V(s)    â”‚
â”‚  3. Policy loss: L_Ï€ = -log_prob(action) * A            â”‚
â”‚  4. Value loss: L_v = MSE(V(s), reward + Î³*V(s'))      â”‚
â”‚  5. Total loss: L = L_Ï€ + 0.5*L_v - 0.01*entropy       â”‚
â”‚  6. Backpropagate: loss.backward()                      â”‚
â”‚  7. Update weights: optimizer.step()                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: TRAINING LOOP                                   â”‚
â”‚                                                          â”‚
â”‚  Episode Complete:                                      â”‚
â”‚  â€¢ Total Reward: 420                                    â”‚
â”‚  â€¢ Steps: 3                                             â”‚
â”‚  â€¢ Success: true                                        â”‚
â”‚  â€¢ Exploration Rate: 0.985 â†’ 0.980                      â”‚
â”‚                                                          â”‚
â”‚  if episode % 100 == 0:                                 â”‚
â”‚      save_checkpoint()                                  â”‚
â”‚                                                          â”‚
â”‚  Next Episode â†’ Go to Step 1                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Interaction Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚   Component       â”‚ Step1â”‚ Step2â”‚ Step3â”‚ Step4â”‚ Step6â”‚ Step8â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ GameStateCollectorâ”‚  âœ“   â”‚  â†’   â”‚      â”‚      â”‚      â”‚      â”‚
â”‚ DataPreprocessor  â”‚  â†   â”‚  âœ“   â”‚  â†’   â”‚      â”‚      â”‚      â”‚
â”‚ StateVector       â”‚      â”‚  â†   â”‚  âœ“   â”‚  â†’   â”‚      â”‚      â”‚
â”‚ MLAgentBrain      â”‚      â”‚      â”‚  â†   â”‚  âœ“   â”‚  â†’   â”‚      â”‚
â”‚ RewardCalculator  â”‚      â”‚      â”‚      â”‚  â†   â”‚  âœ“   â”‚  â†’   â”‚
â”‚ TrainingManager   â”‚  â†•   â”‚      â”‚      â”‚      â”‚  â†   â”‚  âœ“   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Legend: âœ“ Primary   â†’ Sends to   â† Receives from   â†• Both
```

## File Organization

```
AingryBirds/
â”‚
â”œâ”€â”€ Assets/Scripts/
â”‚   â”œâ”€â”€ ML/                          â† NEW: AI/ML System
â”‚   â”‚   â”œâ”€â”€ GameStateCollector.cs      (Step 1)
â”‚   â”‚   â”œâ”€â”€ DataPreprocessor.cs        (Step 2)
â”‚   â”‚   â”œâ”€â”€ StateVectorConverter.cs    (Step 3)
â”‚   â”‚   â”œâ”€â”€ MLAgentBrain.cs            (Steps 4,5,9)
â”‚   â”‚   â”œâ”€â”€ RewardCalculator.cs        (Step 6)
â”‚   â”‚   â”œâ”€â”€ TrainingManager.cs         (Steps 7,8)
â”‚   â”‚   â””â”€â”€ PerformanceMonitor.cs      (Step 10)
â”‚   â”‚
â”‚   â””â”€â”€ [Original Game Scripts]      â† EXISTING
â”‚       â”œâ”€â”€ Bird.cs
â”‚       â”œâ”€â”€ SlingShot.cs
â”‚       â”œâ”€â”€ GameManager.cs
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ ML/                              â† NEW: Python Backend
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ setup_environment.sh
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_agent.py           (Steps 7,8)
â”‚   â”‚   â”œâ”€â”€ policy_network.py        (Neural networks)
â”‚   â”‚   â”œâ”€â”€ unity_environment.py     (Communication)
â”‚   â”‚   â””â”€â”€ config.yaml              (Configuration)
â”‚   â”œâ”€â”€ models/                      (Saved checkpoints)
â”‚   â””â”€â”€ data/logs/                   (Training logs)
â”‚
â””â”€â”€ Documentation/                   â† NEW: Guides
    â”œâ”€â”€ AIML_SETUP_GUIDE.md
    â”œâ”€â”€ QUICKSTART.md
    â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md
    â””â”€â”€ ARCHITECTURE.md              (This file)
```

## Training Flow Chart

```
                    START
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Initialize    â”‚
              â”‚ Environment   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Episode Loop  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ (0 to 10000)  â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                      â”‚                  â”‚
                      â–¼                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
              â”‚ Collect State â”‚          â”‚
              â”‚   (Step 1-3)  â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                      â”‚                  â”‚
                      â–¼                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
              â”‚ Select Action â”‚          â”‚
              â”‚   (Step 4)    â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                      â”‚                  â”‚
                      â–¼                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
              â”‚ Execute in    â”‚          â”‚
              â”‚ Physics       â”‚          â”‚
              â”‚   (Step 5)    â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                      â”‚                  â”‚
                      â–¼                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
              â”‚ Calculate     â”‚          â”‚
              â”‚ Reward        â”‚          â”‚
              â”‚   (Step 6)    â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                      â”‚                  â”‚
                      â–¼                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
              â”‚ Update Policy â”‚          â”‚
              â”‚   (Step 7)    â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                      â”‚                  â”‚
                      â–¼                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
              â”‚ Episode Done? â”‚â”€â”€Noâ”€â”€â”€â”€â”€â”€â”˜
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Yes
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Log Metrics   â”‚
              â”‚ Save Checkpointâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Max Episodes? â”‚â”€â”€Noâ”€â”€â”
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                      â”‚ Yes          â”‚
                      â–¼              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
              â”‚ Training      â”‚      â”‚
              â”‚ Complete!     â”‚      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                    â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Next Episode  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  UNITY (Game Runtime)                    â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           ML Agent Brain                       â”‚    â”‚
â”‚  â”‚           (Deployment Mode)                    â”‚    â”‚
â”‚  â”‚                                                 â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚   â”‚ 1. Observe Game State               â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Collect from GameStateCollectorâ”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Process with DataPreprocessor  â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Convert to StateVector         â”‚     â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                  â”‚                             â”‚    â”‚
â”‚  â”‚                  â–¼                             â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚   â”‚ 2. Load Trained Model               â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ ONNX Runtime                   â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Model: trained_model.onnx      â”‚     â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                  â”‚                             â”‚    â”‚
â”‚  â”‚                  â–¼                             â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚   â”‚ 3. Predict Action                   â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Input: state_vector[64]        â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Output: [angle, force]         â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ No exploration (exploit only)  â”‚     â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                  â”‚                             â”‚    â”‚
â”‚  â”‚                  â–¼                             â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚   â”‚ 4. Execute Action                   â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Control Slingshot              â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Launch Bird                    â”‚     â”‚    â”‚
â”‚  â”‚   â”‚    â€¢ Real-time Gameplay             â”‚     â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  Performance Monitor (Optional)                         â”‚
â”‚  â€¢ Track success rate                                   â”‚
â”‚  â€¢ Monitor for degradation                              â”‚
â”‚  â€¢ Recommend retraining if needed                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Reference

### Key Dimensions

- **State Vector**: 64 dimensions
- **Action Space**: 2 dimensions (angle, force)
- **Hidden Layers**: 256, 256, 128 neurons
- **Training Episodes**: 10,000 (configurable)
- **Replay Buffer**: 100,000 experiences

### Key Files

- **Unity ML**: `Assets/Scripts/ML/*.cs`
- **Python Training**: `ML/training/*.py`
- **Configuration**: `ML/training/config.yaml`
- **Models**: `ML/models/`

### Key Commands

```bash
# Setup
./ML/setup_environment.sh

# Train
python ML/training/train_agent.py

# Test
python ML/training/test_agent.py --model MODEL_PATH

# Monitor
tensorboard --logdir=ML/data/logs
```

---

**This architecture implements a complete, production-ready RL system for autonomous Angry Birds gameplay! ğŸ®ğŸ¤–**
